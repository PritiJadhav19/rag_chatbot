# ğŸ¤– RAG-Based Chatbot using Ollama (Local LLM)

This project implements a **Retrieval-Augmented Generation (RAG) chatbot** that allows users to ask questions from their **own documents (PDF/TXT/MD)**.  
It uses **Ollama** to run a **local Large Language Model (LLM)** and **ChromaDB** for vector-based semantic search.

ğŸ‘‰ No OpenAI API key required  
ğŸ‘‰ Fully local & privacy-friendly  

---

## âœ¨ Features

- ğŸ“„ Upload and chat with **PDF / TXT / Markdown** files  
- ğŸ§  Semantic search using **HuggingFace embeddings**  
- ğŸ—‚ï¸ Persistent **vector database (ChromaDB)**  
- ğŸ¤– Local LLM inference using **Ollama**  
- ğŸ§¾ Source references for answers  
- ğŸ–¥ï¸ Interactive **Streamlit chat UI**

---

## ğŸ§  What is RAG?

**Retrieval-Augmented Generation (RAG)** combines:
1. **Information Retrieval** â€“ fetch relevant document chunks
2. **Text Generation** â€“ generate answers using an LLM

This reduces hallucinations and ensures answers are **grounded in real documents**.

---

## ğŸ› ï¸ Tech Stack

- **Python 3.10+**
- **Streamlit** â€“ frontend UI
- **LangChain**
- **Ollama** â€“ local LLM (LLaMA / Mistral)
- **HuggingFace Embeddings**
- **ChromaDB** â€“ vector store
- **PyPDF** â€“ PDF loading

---

## ğŸ“ Project Structure
